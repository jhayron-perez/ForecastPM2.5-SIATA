{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f7e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ecCodes 2.21.0 or higher is recommended. You are running version 2.19.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '20'\n",
    "os.environ['MKL_NUM_THREADS'] = '20'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import datetime as dt\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "import xarray as xr\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import signal\n",
    "import multiprocessing\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from scipy import interpolate\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'/home/jsperezc/jupyter/AQ_Forecast/functions/')\n",
    "import postprocessing\n",
    "from chained_models import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import RegressorChain, MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from denseweight import DenseWeight\n",
    "# from gbdtmo import GBDTMulti, load_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350b3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_esta= pd.read_csv(\"/var/data1/AQ_Forecast_DATA/historic/PM25/CoordenadasEstaciones.csv\", index_col= \"Nombre\")\n",
    "\n",
    "\n",
    "Estaciones = [\"ITA-CJUS\", \"CAL-LASA\", \"ITA-CONC\", \"MED-LAYE\", \"CAL-JOAR\", \"EST-HOSP\", \"MED-ALTA\", \"MED-VILL\", \"BAR-TORR\", \n",
    "              \"COP-CVID\", \"MED-BEME\", \"MED-TESO\", \"MED-SCRI\", \"MED-ARAN\", \"BEL-FEVE\", \"ENV-HOSP\", \"SAB-RAME\", \"MED-SELE\",\n",
    "             \"CEN-TRAF\",\"SUR-TRAF\"]\n",
    "\n",
    "\n",
    "#coor_esta = coor_esta.drop(short_stations,axis = 0)\n",
    "# coor_esta = coor_esta.loc[Estaciones]\n",
    "\n",
    "file_gfs_ref = os.listdir(\"/var/data1/AQ_Forecast_DATA/historic/GFS/historic/00/\")[0]\n",
    "gfs_referencia = xr.open_dataset(\"/var/data1/AQ_Forecast_DATA/historic/GFS/historic/00/\" + file_gfs_ref)  \n",
    "gfs_referencia = postprocessing.recorte_espacial(gfs_referencia)\n",
    "\n",
    "max_lag = 96 ## Maximum lag used\n",
    "n_leadtime = 96\n",
    "keys_gfs = ['tcc','rad','prate','hpbl','cin']\n",
    "\n",
    "def clean_abrupt_gradient(anoms):\n",
    "    diff = abs(np.diff(anoms))\n",
    "    anoms[np.hstack([diff>=1.5,np.array([False])])] = np.nan\n",
    "    anoms[np.hstack([np.array([False]),diff>=1.5])] = np.nan\n",
    "    return pd.Series(anoms,index = anoms.index)\n",
    "\n",
    "def get_optimal_window(variable,hour):\n",
    "    return np.argmax(abs(np.load('/var/data1/AQ_Forecast_DATA/historic/GFS/correlations/CorrsAnomsGFS_'\\\n",
    "                                 +variable+'_v3.npy')[:,hour]))+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9819d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        grad = 2 * (y_pred - y_true)\n",
    "        hess = 2 * np.ones_like(y_pred)\n",
    "        return grad, hess\n",
    "\n",
    "class CustomWeightedMSELoss:\n",
    "    def __init__(self, weighting_function):\n",
    "        self.weighting_function = weighting_function\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Compute the weighted differences\n",
    "        weighted_diff = self.weighting_function(y_true) * (y_pred - y_true)\n",
    "        # Compute the weighted MSE loss\n",
    "        grad = 2 * weighted_diff\n",
    "        hess = 2 * self.weighting_function(y_true)\n",
    "        \n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68557a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CAMS\n",
    "\n",
    "data_CAMS = postprocessing.call_files('CAMS')\n",
    "data_CAMS = postprocessing.recorte_espacial(data_CAMS)\n",
    "\n",
    "dataset_full = data_CAMS\n",
    "\n",
    "lat_cams = dataset_full.latitude.values\n",
    "lon_cams = dataset_full.longitude.values\n",
    "\n",
    "aod = np.nanmean(dataset_full.aod550.values,axis = (1,2))#[:].data\n",
    "bcaod = np.nanmean(dataset_full.bcaod550.values,axis = (1,2))#[:].data\n",
    "pm2p5_cams = np.nanmean(dataset_full.pm2p5.values,axis = (1,2)) #* 1000_000_000\n",
    "\n",
    "df_cams = pd.DataFrame(np.array([aod,bcaod,pm2p5_cams]).T,index= dataset_full.time.values,\n",
    "    columns = ['aod','bcaod','pm2p5_cams'])\n",
    "# df_cams = pd.DataFrame(np.array([aod,pm2p5_cams]).T,index= dataset_full.time.values,\n",
    "#     columns = ['aod','pm2p5_cams'])\n",
    "# df_cams = (df_cams.rolling(3,center=True).mean())\n",
    "df_cams.index = df_cams.index - dt.timedelta(hours = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0803386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cams.to_csv('/var/data1/AQ_Forecast_DATA/zenodo/df_cams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1de08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pm25 = pd.read_csv('/var/data1/AQ_Forecast_DATA/historic/PM25/FullPM2.5.csv',index_col=0,parse_dates=True)\n",
    "short_stations = ['SAB-JOFE','MED-UNNV','GIR-TEMP','GIR-SOSN','GIR-EPM','MED-FISC']\n",
    "# data_pm25.index = data_pm25.index-dt.timedelta(hours = 1) \n",
    "data_pm25 = data_pm25.drop(short_stations,axis = 1)\n",
    "stations = np.sort(data_pm25.keys())\n",
    "data_pm25 = data_pm25['2017-10-01':'2020-04-01']\n",
    "data_pm25 = data_pm25.interpolate(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63585b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifrp = pd.read_csv(\"/var/data1/AQ_Forecast_DATA/historic/Fires/Processed/IFRP_Radios_800hpa_4dias.csv\", \n",
    "                   usecols=[str(i) for i in np.arange(16)],index_col=0,parse_dates=True)\n",
    "ifrp.columns = np.arange(50, 800, 50).astype(str)\n",
    "\n",
    "ifrp_optimal_50 = ifrp[['50']].rolling(24).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c24ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = {\n",
    "    'gb_ch':\n",
    "         MultiOutputLGBM(**{\n",
    "            'task': 'train', \n",
    "            'num_leaves': 5,\n",
    "            'min_child_samples':150,\n",
    "            'max_depth':30,\n",
    "            'extra_trees':'true',\n",
    "            'learning_rate':0.05,\n",
    "            'n_jobs': 20,\n",
    "            'linear_tree': True,\n",
    "            'verbose': -1}),\n",
    "    'gb_mo':\n",
    "         MultiOutputRegressor(lgb.LGBMRegressor(**{'task': 'train',\n",
    "            'num_leaves': 5,\n",
    "            'min_child_samples': 150,\n",
    "            'max_depth':30,\n",
    "            'extra_trees': 'true',\n",
    "            'learning_rate':0.05,\n",
    "            'n_jobs': 1,                               \n",
    "            'verbose': -1,\n",
    "            'importance_type': 'gain',\n",
    "            'linear_tree': True,\n",
    "            }), n_jobs=20),\n",
    "    \n",
    "    'rf_ch':\n",
    "         MultiOutputRF(**{'bootstrap': True,\n",
    "             'max_samples': 0.2,\n",
    "             'max_depth': 10,\n",
    "             'max_features': 10,\n",
    "             'n_estimators': 50,\n",
    "             'n_jobs': 20}),\n",
    "    'rf_mo':\n",
    "         MultiOutputRegressor(RandomForestRegressor(**{'bootstrap': True,\n",
    "         'max_samples': 0.4,\n",
    "         'max_depth': 10,\n",
    "         'max_features': 80,\n",
    "         'n_estimators': 50,\n",
    "         'n_jobs': 1}), n_jobs=20),\n",
    "    \n",
    "    'lr_rc':\n",
    "         MultiOutputLR(**{'n_jobs': 1}),\n",
    "    \n",
    "    'lr_mo':\n",
    "         MultiOutputRegressor(Ridge(), n_jobs=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba662ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "def train_estimator(X,Y,estimator,path_scaler_x=None,path_scaler_y=None,path_estimator=None,weighted_loss=False):\n",
    "    #Scale\n",
    "    scaler_x = RobustScaler().fit(X)\n",
    "    scaler_y = RobustScaler().fit(Y)\n",
    "    X_scaled = scaler_x.transform(X)\n",
    "    Y_scaled = scaler_y.transform(Y) \n",
    "    \n",
    "    if path_scaler_x!=None:\n",
    "        joblib.dump(scaler_x, open(path_scaler_x, 'wb'))\n",
    "    if path_scaler_y!=None:\n",
    "        joblib.dump(scaler_y, open(path_scaler_y, 'wb'))\n",
    "#     print(X_scaled)\n",
    "    if weighted_loss==True:\n",
    "        # Define DenseWeight\n",
    "        dw = DenseWeight(alpha=1.15)\n",
    "        weights_temp = dw.fit(Y_scaled[:,0])\n",
    "        print(path_estimator)\n",
    "        if '_MO' in path_estimator:\n",
    "            estimator.estimator.set_params(objective=CustomWeightedMSELoss(dw))\n",
    "        elif '_CH' in path_estimator:\n",
    "            estimator.kwargs['objective'] = CustomWeightedMSELoss(dw)\n",
    "    estimator.fit(X_scaled,Y_scaled)\n",
    "    \n",
    "    if path_estimator!=None:\n",
    "        with open(path_estimator, 'wb') as f:\n",
    "            dill.dump(estimator, f)\n",
    "        \n",
    "    return estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686035d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_estimator(X,Y,estimator,n_splits = 5,gap=96,weighted_loss=False,name_estimator = ''):\n",
    "    #Scale\n",
    "    scaler_x = RobustScaler().fit(X)\n",
    "    scaler_y = RobustScaler().fit(Y)\n",
    "    X_scaled = scaler_x.transform(X)\n",
    "    Y_scaled = scaler_y.transform(Y) \n",
    "    \n",
    "    #Get splits\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
    "    \n",
    "    mses=[]\n",
    "    for train_cv, test_cv in tscv.split(X=X_scaled):\n",
    "        X_train_temp = X_scaled[train_cv]\n",
    "        Y_train_temp = Y_scaled[train_cv]\n",
    "\n",
    "        X_test_temp = X_scaled[test_cv]\n",
    "        Y_test_temp = Y_scaled[test_cv]\n",
    "        if weighted_loss==True:\n",
    "            # Define DenseWeight\n",
    "            dw = DenseWeight(alpha=1.15)\n",
    "            weights_temp = dw.fit(Y_scaled[:,0])\n",
    "            if '_MO' in name_estimator:\n",
    "                estimator.estimator.set_params(objective=CustomWeightedMSELoss(dw))\n",
    "            elif '_CH' in name_estimator:\n",
    "                estimator.kwargs['objective'] = CustomWeightedMSELoss(dw)\n",
    "        estimator.fit(X_train_temp,Y_train_temp)\n",
    "        y_model = estimator.predict(X_test_temp)\n",
    "#         y_model_full = estimator.predict_random(X_test_temp)\n",
    "#         for j in range(y_model_full.shape[0]):\n",
    "#             y_model_full[j] = scaler_y.inverse_transform(y_model_full[j])\n",
    "        y_model = scaler_y.inverse_transform(y_model)\n",
    "        y_val = scaler_y.inverse_transform(Y_test_temp)\n",
    "    \n",
    "        mses.append(mean_squared_error(y_model,y_val))\n",
    "#     return y_val,y_model, y_model_full, np.mean(mses)\n",
    "    return np.mean(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb21a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITA-CJUS\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ITA-CJUS_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ITA-CJUS_GB_CH.mdl\n",
      "CAL-LASA\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CAL-LASA_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CAL-LASA_GB_CH.mdl\n",
      "ITA-CONC\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ITA-CONC_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ITA-CONC_GB_CH.mdl\n",
      "MED-LAYE\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-LAYE_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-LAYE_GB_CH.mdl\n",
      "CAL-JOAR\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CAL-JOAR_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CAL-JOAR_GB_CH.mdl\n",
      "EST-HOSP\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/EST-HOSP_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/EST-HOSP_GB_CH.mdl\n",
      "MED-ALTA\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-ALTA_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-ALTA_GB_CH.mdl\n",
      "MED-VILL\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-VILL_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-VILL_GB_CH.mdl\n",
      "BAR-TORR\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/BAR-TORR_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/BAR-TORR_GB_CH.mdl\n",
      "COP-CVID\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/COP-CVID_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/COP-CVID_GB_CH.mdl\n",
      "MED-BEME\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-BEME_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-BEME_GB_CH.mdl\n",
      "MED-TESO\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-TESO_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-TESO_GB_CH.mdl\n",
      "MED-SCRI\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-SCRI_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-SCRI_GB_CH.mdl\n",
      "MED-ARAN\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-ARAN_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-ARAN_GB_CH.mdl\n",
      "BEL-FEVE\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/BEL-FEVE_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/BEL-FEVE_GB_CH.mdl\n",
      "ENV-HOSP\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ENV-HOSP_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/ENV-HOSP_GB_CH.mdl\n",
      "SAB-RAME\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/SAB-RAME_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/SAB-RAME_GB_CH.mdl\n",
      "MED-SELE\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-SELE_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/MED-SELE_GB_CH.mdl\n",
      "CEN-TRAF\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CEN-TRAF_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/CEN-TRAF_GB_CH.mdl\n",
      "SUR-TRAF\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/SUR-TRAF_GB_MO.mdl\n",
      "/var/data1/AQ_Forecast_DATA/trained_estimators_v3/SUR-TRAF_GB_CH.mdl\n"
     ]
    }
   ],
   "source": [
    "### To validation errors dataframe:\n",
    "\n",
    "station_names = []\n",
    "estimator_names = []\n",
    "cv_error = []\n",
    "\n",
    "for station_i in Estaciones[:]:\n",
    "# for station_i in ['ENV-HOSP']:\n",
    "    print(station_i)\n",
    "    df_mean = pd.DataFrame(data_pm25[station_i])\n",
    "    df_mean.columns = [\"PM25_stations\"]\n",
    "    \n",
    "    ## Definición de la serie de tiempo de cams para cada estación (un sólo pixel)\n",
    "    lat_cams = dataset_full.latitude.values\n",
    "    lon_cams = dataset_full.longitude.values\n",
    "    \n",
    "    ## Station coordinates\n",
    "    lat_est = coor_esta.loc[station_i].Latitud\n",
    "    lon_est = coor_esta.loc[station_i].Longitud\n",
    "    \n",
    "    df_hourly_chem = pd.concat([df_mean,df_cams],axis=1).dropna()\n",
    "    ###Read GFS###################\n",
    "    \n",
    "    latitudes, longitudes = np.array(lat_est), np.array(lon_est)\n",
    "    xs_gfs, ys_gfs = np.meshgrid(gfs_referencia.longitude, gfs_referencia.latitude)\n",
    "    arbol_binario = spatial.cKDTree(np.c_[xs_gfs.ravel(), ys_gfs.ravel()])\n",
    "    \n",
    "    lxs, lys = np.meshgrid(longitudes, latitudes)\n",
    "    distancia, indice = arbol_binario.query([longitudes, latitudes])\n",
    "        \n",
    "    lon_file = np.c_[xs_gfs.ravel(), ys_gfs.ravel()][indice][0]\n",
    "    lat_file = np.c_[xs_gfs.ravel(), ys_gfs.ravel()][indice][1]\n",
    "    \n",
    "    df_GFS_hourly = pd.read_csv(f'/var/data1/AQ_Forecast_DATA/historic/GFS/pixel_series/GFS_{lat_file}_{lon_file}.csv',index_col = 0, parse_dates=True)\n",
    "    df_GFS_hourly.index = df_GFS_hourly.index - dt.timedelta(hours = 1)\n",
    "    \n",
    "    ### PREPARA DATOS DE ENTRADA\n",
    "\n",
    "    df_optimal_cams = pd.DataFrame(index = df_hourly_chem.index)\n",
    "    df_optimal_cams['aod'] = df_hourly_chem.aod.rolling(3).mean()\n",
    "    df_optimal_cams['bcaod'] = df_hourly_chem.bcaod.rolling(1).mean()\n",
    "    df_optimal_cams['pm2p5_cams'] = df_hourly_chem.pm2p5_cams.rolling(3).mean()\n",
    "\n",
    "    df_optimal_gfs = pd.DataFrame(index = df_GFS_hourly.index)\n",
    "\n",
    "    for variable in keys_gfs:\n",
    "        df_optimal_temp = copy.deepcopy(df_GFS_hourly[[variable]])\n",
    "        for hour in range(0,24):\n",
    "            optimal_window = get_optimal_window(variable,hour)\n",
    "            df_optimal_temp[df_optimal_temp.index.hour == hour] = df_GFS_hourly[[variable]].rolling(optimal_window).mean()\\\n",
    "                [df_optimal_temp.index.hour == hour]\n",
    "        df_optimal_gfs[variable] = df_optimal_temp\n",
    "\n",
    "    df_hour = pd.DataFrame(df_hourly_chem.index.hour, index = df_hourly_chem.index)\n",
    "    df_hour1 = np.sin(2*np.pi*(df_hour/24))\n",
    "    df_hour2 = np.cos(2*np.pi*(df_hour/24))\n",
    "\n",
    "    df_dow = pd.DataFrame(df_hourly_chem.index.dayofweek, index = df_hourly_chem.index)\n",
    "    df_dow1 = np.sin(2*np.pi*(df_dow/7))\n",
    "    df_dow2 = np.cos(2*np.pi*(df_dow/7))\n",
    "    \n",
    "    max_lag = 48\n",
    "    x_shifts = pd.concat([df_hourly_chem.PM25_stations.shift(i) for i in range(max_lag,0,-1)],axis = 1).dropna()\n",
    "    x_shifts.columns = np.array(['PM_'+str(i).zfill(2) for i in range(max_lag,0,-1)])\n",
    "\n",
    "    aod_future = pd.DataFrame(np.array([df_optimal_cams[['aod']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_cams[['aod']].index,\\\n",
    "                 columns = np.array(['AOD_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    bc_future = pd.DataFrame(np.array([df_optimal_cams[['bcaod']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_cams[['bcaod']].index,\\\n",
    "                 columns = np.array(['BCAOD_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    pm2p5_future = bc_future = pd.DataFrame(np.array([df_optimal_cams[['pm2p5_cams']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_cams[['pm2p5_cams']].index,\\\n",
    "                 columns = np.array(['PMCAMS_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "\n",
    "    tcc_future = pd.DataFrame(np.array([df_optimal_gfs[['tcc']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_gfs[['tcc']].index,\\\n",
    "                 columns = np.array(['tcc_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    rad_future = pd.DataFrame(np.array([df_optimal_gfs[['rad']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_gfs[['rad']].index,\\\n",
    "                 columns = np.array(['rad_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    prate_future = pd.DataFrame(np.array([df_optimal_gfs[['prate']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_gfs[['prate']].index,\\\n",
    "                 columns = np.array(['prate_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    hpbl_future = pd.DataFrame(np.array([df_optimal_gfs[['hpbl']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_gfs[['hpbl']].index,\\\n",
    "                 columns = np.array(['hpbl_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "    cin_future = pd.DataFrame(np.array([df_optimal_gfs[['cin']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = df_optimal_gfs[['cin']].index,\\\n",
    "                 columns = np.array(['cin_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "\n",
    "\n",
    "    ifrp50_future = pd.DataFrame(np.array([ifrp_optimal_50[['50']].shift(-i).values for i in range(0,97,3)])[:,:,0].T,\\\n",
    "                 index = ifrp_optimal_50[['50']].index,\\\n",
    "                 columns = np.array(['ifrp50_'+str(i).zfill(2) for i in range(0,97,3)]))\n",
    "\n",
    "    dow1_future = pd.DataFrame(np.array([df_dow1[[0]].shift(-i).values for i in range(0,96,24)])[:,:,0].T,\\\n",
    "                 index = df_dow1[[0]].index,\\\n",
    "                 columns = np.array(['dow1_'+str(i).zfill(2) for i in range(0,96,24)]))\n",
    "    dow2_future = pd.DataFrame(np.array([df_dow2[[0]].shift(-i).values for i in range(0,96,24)])[:,:,0].T,\\\n",
    "                 index = df_dow2[[0]].index,\\\n",
    "                 columns = np.array(['dow2_'+str(i).zfill(2) for i in range(0,96,24)]))\n",
    "\n",
    "    hod1_future = pd.DataFrame(np.array([df_hour1[[0]].shift(-i).values for i in range(0,24,1)])[:,:,0].T,\\\n",
    "                 index = df_hour1[[0]].index,\\\n",
    "                 columns = np.array(['hod1_'+str(i).zfill(2) for i in range(0,24,1)]))\n",
    "    hod2_future = pd.DataFrame(np.array([df_hour2[[0]].shift(-i).values for i in range(0,24,1)])[:,:,0].T,\\\n",
    "                 index = df_hour2[[0]].index,\\\n",
    "                 columns = np.array(['hod2_'+str(i).zfill(2) for i in range(0,24,1)]))\n",
    "\n",
    "    y_future = pd.DataFrame(np.array([df_hourly_chem[['PM25_stations']].shift(-i).values for i in range(0,96)])[:,:,0].T,\\\n",
    "                 index = df_hourly_chem[['PM25_stations']].index,\\\n",
    "                 columns = np.array(['y_'+str(i).zfill(2) for i in range(0,96)]))\n",
    "    \n",
    "    ### try rolling mean output # this was a winner\n",
    "    pm_smoothed = df_hourly_chem[['PM25_stations']].rolling(26,min_periods=16).mean()\n",
    "    y_future_smoothed = pd.DataFrame(np.array([pm_smoothed.shift(-i).values for i in range(0,96)])[:,:,0].T,\\\n",
    "                 index = pm_smoothed.index,\\\n",
    "                 columns = np.array(['y_'+str(i).zfill(2) for i in range(0,96)]))\n",
    "    \n",
    "    X = pd.concat([x_shifts,\n",
    "    aod_future,\n",
    "#     bc_future, NO USAR NUNCA, DAÑAN EL PRONÓSTICO :C\n",
    "#     pm2p5_future,\n",
    "    tcc_future,\n",
    "#     rad_future, meh\n",
    "    prate_future,\n",
    "    hpbl_future,\n",
    "#     cin_future, meh\n",
    "    ifrp50_future,\n",
    "    dow1_future,\n",
    "    dow2_future,\n",
    "    hod1_future,\n",
    "    hod2_future],axis=1).dropna()\n",
    "\n",
    "    Y = y_future_smoothed.loc[X.index] \n",
    "    XY = pd.concat([X,Y],axis=1).dropna()\n",
    "    X = XY.iloc[:,:-96]\n",
    "    Y = XY.iloc[:,-96:]\n",
    "#     for estimator_name in base_estimators.keys():\n",
    "    for estimator_name in ['gb_mo','gb_ch']:    \n",
    "#         try:\n",
    "        if ('gb_mo' in estimator_name)|('gb_ch' in estimator_name):\n",
    "            estimator_temp = copy.deepcopy(base_estimators[estimator_name])\n",
    "            train_estimator(X,Y,estimator_temp,\n",
    "             '/var/data1/AQ_Forecast_DATA/scalers_v3/'+station_i+'_X.scl',\n",
    "             '/var/data1/AQ_Forecast_DATA/scalers_v3/'+station_i+'_Y.scl',\n",
    "             '/var/data1/AQ_Forecast_DATA/trained_estimators_v3/'+station_i+'_'+str(estimator_name.upper())+'.mdl',weighted_loss=True)\n",
    "#                 error_temp = np.sqrt(cross_validate_estimator(X,Y, estimator_temp,weighted_loss=True,name_estimator=estimator_name))\n",
    "#         #         aaaaa\n",
    "#                 station_names.append(station_i)\n",
    "#                 estimator_names.append(estimator_name)\n",
    "#                 cv_error.append(error_temp)\n",
    "#                 print(estimator_name,str(error_temp))\n",
    "        else:\n",
    "            estimator_temp = copy.deepcopy(base_estimators[estimator_name])\n",
    "            train_estimator(X,Y,estimator_temp,\n",
    "             '/var/data1/AQ_Forecast_DATA/scalers_v3/'+station_i+'_X.scl',\n",
    "             '/var/data1/AQ_Forecast_DATA/scalers_v3/'+station_i+'_Y.scl',\n",
    "             '/var/data1/AQ_Forecast_DATA/trained_estimators_v3/'+station_i+'_'+str(estimator_name.upper())+'.mdl',weighted_loss=False)\n",
    "#                 error_temp = np.sqrt(cross_validate_estimator(X,Y, estimator_temp,weighted_loss=False,name_estimator=estimator_name))\n",
    "#         #         aaaaa\n",
    "#                 station_names.append(station_i)\n",
    "#                 estimator_names.append(estimator_name)\n",
    "#                 cv_error.append(error_temp)\n",
    "#                 print(estimator_name,str(error_temp))\n",
    "\n",
    "#         except Exception as e: \n",
    "#             print(e)\n",
    "#             station_names.append(station_i)\n",
    "#             estimator_names.append(estimator_name)\n",
    "#             cv_error.append(np.nan)\n",
    "#         aaaa    \n",
    "#         df_errors = pd.DataFrame(columns = ['station','estimator','error'])    \n",
    "#         df_errors['station'] = station_names\n",
    "#         df_errors['estimator'] = estimator_names\n",
    "#         df_errors['error'] = cv_error\n",
    "#         df_errors.to_csv('/var/data1/AQ_Forecast_DATA/errors/cv_stations_v3.csv')  \n",
    "#         aaaa\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsperezc-Python3",
   "language": "python",
   "name": "jsperezc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
